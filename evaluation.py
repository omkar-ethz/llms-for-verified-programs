"""Contains functions to get various prompt combinations"""
import random
import time
from typing import Literal
import dataclasses
import openai
import data
import nagini_client as nagini
import model


@dataclasses.dataclass
class EvalResult:
    """Class to store the results of the evaluation"""

    results: dict[str, bool | Literal["Timeout"]] = dataclasses.field(
        default_factory=dict
    )
    verified_at: dict[str, tuple[int, int]] = dataclasses.field(default_factory=dict)


class Evaluation:
    """Evaluations of model on dataset specified in config_file"""

    def __init__(
        self,
        dataset: data.Data,
        model_str: model.GPTModel = "gpt-3.5-turbo-1106",
    ):
        self.data = dataset
        self.model = model.get_model(model_str, self.data)

    @classmethod
    def from_config(
        cls, config_file: str, model_str: model.GPTModel = "gpt-3.5-turbo-1106"
    ):
        """Creates an evaluation from the given config file"""
        return cls(data.Data(config_file), model_str)

    def verify_program_snippet(
        self, key: str, program_snippet: str
    ) -> nagini.VerificationResult:
        """Expects: response contains a candidate program generated by assistant\n
        Sends the program to Nagini and returns the verification result"""
        program_file = self.data.combine_method_with_declaration(key, program_snippet)
        result = nagini.verify(program_file)
        for i, line_no in enumerate(result.line_no):
            result.line_no[i] = self._get_relative_line_number(key, line_no)
        return result

    def _get_relative_line_number(self, key: str, line_no: str) -> str:
        """Returns the line number relative to the method"""
        declaration = self.data.get_declaration(key)
        line, column = line_no.split(".")
        rel_line = int(line) - len(declaration.split("\n"))
        return f"{rel_line}.{column}"

    def run_eval(
        self,
        k=1,
        n=1,
        key="list",
        with_errors=True,
    ) -> EvalResult:
        """Runs the evaluation for all examples in the list of examples for the given key\n"""
        examples = self.data.get_list_of_examples(key)
        # examples = ["prepend", "join_lists"]
        eval_result = EvalResult()
        for example in examples:
            result, verified_at = self.run_example(example, k, n, key, with_errors)
            eval_result.results[example] = result
            if verified_at is not None:
                eval_result.verified_at[example] = verified_at
        return eval_result

    def run_example(
        self, example: str, k=1, n=1, key="list", with_errors=True
    ) -> tuple[bool | Literal["Timeout"], tuple[int, int] | None]:
        """Runs the evaluation for a single given example"""
        result: bool | Literal["Timeout"]
        verified_at: tuple[int, int] | None = None
        rand = random.Random(42)
        for i in range(k):
            seed = rand.randint(12345, 54321)
            prompt = self.model.get_prompt(example, with_errors=with_errors)
            for j in range(n):
                print(
                    "Running example:",
                    example,
                    "; attempt:",
                    i + 1,
                    "; error depth:",
                    j + 1,
                )
                try:
                    response = self.model.get_response(prompt, seed=seed)
                except openai.APITimeoutError as e:
                    result = "Timeout"
                    print("Timeout error!", e)
                    continue
                program_snippet = self.model.print_and_process_response(response)
                verif_result = self.verify_program_snippet(key, program_snippet)
                print("Verification result:\n", verif_result, "\n\n")
                result = verif_result.status == "Verification successful"
                if result:
                    verified_at = (i + 1, j + 1)
                    break
                if n > 1:
                    self.model.extend_prompt(prompt, program_snippet, str(verif_result))
                time.sleep(5)
            if result:
                break
        return result, verified_at


# print(get_few_shot_prompt("prepend"))
# print(run_single("list", data.get_example("list", "join_lists", "unverified")))
